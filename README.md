# Object-Detection-of-Insects-Caught-by-Yellow-Sticky-Traps
Comparative Analysis of EfficientDet, and YOLO for Object Detection of Insects Caught by Yellow Sticky Traps

This repository contains the code and notebooks for the project "Comparative Analysis of EfficientDet, and YOLO for Object Detection," by Anne Pereira Collier de Mendon√ßa and Alice Peruniz Oliveira.

**Project Overview**

This project presents a comparative study of single-stage object detectors, specifically EfficientDet and YOLO, for the task of detecting insects on yellow sticky traps. Using the "Yellow Sticky Traps" dataset, which contains over 8,000 labeled instances across three pest species, the study evaluates each architecture's performance in a challenging scenario characterized by small object size and high density.

The analysis demonstrates that while YOLOv4 establishes a functional baseline and EfficientDet shows potential for real-time inference, YOLOv8 delivers significantly superior performance in terms of accuracy (mAP), making it the most effective solution for this application.

**Dataset**

Name: Yellow Sticky Traps (https://github.com/md-121/yellow-sticky-traps-dataset) 

Images: 284 images, split into 227 for training and 57 for validation (approx. 80/20 ratio).

Classes (3):
- MR: Macrolophus pygmaeus
- NC: Nesidiocoris tenuis
- WF: Trialeurodes vaporariorum
  
Annotations: The original annotations are in PASCAL-VOC XML format. The preprocessing pipelines in the notebooks convert these annotations into the required formats for each framework.

**Results Summary**

- YoloV4: Moderate/low performance, with difficulty detecting the 'WF' class.
- EfficientDet: Failed to learn object localization (stagnant regression loss).
- YOLOv8: Superior performance and converged successfully across all classes.

**Conclusion**

The comparative analysis demonstrated that YOLOv8 is the most robust and accurate architecture for this insect detection task. Its ability to generalize from pre-trained weights and its modern design far surpassed the performance of YOLOv4 and overcame the training challenges faced by EfficientDet in this specific context.

**Files in this Repository**

YOLOv8.ipynb: Notebook for training and validating the YOLOv8n model using the Ultralytics framework. It handles the dataset download, annotation conversion, and the complete training and validation cycle

YOLOv4.ipynb: Notebook for training and validating the YOLOv4 model using the AlexeyAB/darknet framework. This notebook clones the Darknet repository, prepares the dataset, and trains the YOLOv4 model. Key hyperparameters are defined within the yolov4_custom.cfg file generated by the notebook.

EfficientDet.ipynb: Notebook for training and evaluating the EfficientDet-D0 model using PyTorch. It demonstrates the data preparation process (including conversion to COCO format) and the two-stage training cycle (head-only training phase and the full-model fine-tuning).

requirements.txt: A list of all the Python packages required to run the notebooks.

# Installation & Quick Run
All experiments are designed to be run in an environment like Google Colab with GPU acceleration enabled.

Clone the repository:

`git clone https://github.com/annecollier/Object-Detection-of-Insects-Caught-by-Yellow-Sticky-Traps.git`

`cd Object-Detection-of-Insects-Caught-by-Yellow-Sticky-Traps`

Install dependencies:

`pip install -r requirements.txt`

Run an experiment:

Open one of the notebooks in Google Colab Pro.

Ensure the runtime type is set to GPU.

Execute all cells from top to bottom to replicate the training and evaluation process.

# References and Acknowledgements
This project was built upon several outstanding open-source frameworks and public datasets.

Dataset: The Yellow Sticky Traps Dataset was used for all training and evaluation.

YOLOv8: We use the official framework provided by Ultralytics.

YOLOv4: The YOLOv4 experiments are implemented using the popular fork of the Darknet framework by AlexeyAB.

EfficientDet: The EfficientDet implementation is based on a widely used PyTorch version.
