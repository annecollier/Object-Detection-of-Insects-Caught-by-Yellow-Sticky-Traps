# -*- coding: utf-8 -*-
"""YoloV8_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17tnwzgpzcH5i8c9pHosAf3wjI1h9EbTo
"""

# Setup and Imports

# Install the Ultralytics library, which contains the YOLOv8 framework
!pip install -q ultralytics

# Import all required libraries
import os
import shutil
import random
import yaml
import cv2
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import xml.etree.ElementTree as ET
import time
from glob import glob
from ultralytics import YOLO
from IPython.display import Image as IPImage, display
from PIL import Image
from tqdm.notebook import tqdm
import torch

print("Setup Complete")

# Dataset Download, Conversion, and Structuring

if not os.path.exists('/content/yellow-sticky-traps-dataset'):
    !git clone -q https://github.com/md-121/yellow-sticky-traps-dataset
else:
    print("Dataset repository already cloned")

# Define Paths and Create Clean YOLO-style Directory Structure
SOURCE_IMAGES_DIR = '/content/yellow-sticky-traps-dataset/images'
SOURCE_ANNOTATIONS_DIR = '/content/yellow-sticky-traps-dataset/annotations'
YOLO_DATASET_DIR = '/content/yolo_dataset'
if os.path.exists(YOLO_DATASET_DIR):
    shutil.rmtree(YOLO_DATASET_DIR)
PATHS = {
    'train_images': os.path.join(YOLO_DATASET_DIR, 'images/train'),
    'val_images': os.path.join(YOLO_DATASET_DIR, 'images/val'),
    'train_labels': os.path.join(YOLO_DATASET_DIR, 'labels/train'),
    'val_labels': os.path.join(YOLO_DATASET_DIR, 'labels/val')
}
for path in PATHS.values():
    os.makedirs(path, exist_ok=True)

# Find and Match All Image/Annotation Pairs
all_image_paths = glob(os.path.join(SOURCE_IMAGES_DIR, '*.jpg'))
image_basenames = {os.path.splitext(os.path.basename(p))[0] for p in all_image_paths}
all_annot_paths = glob(os.path.join(SOURCE_ANNOTATIONS_DIR, '*.xml'))
valid_pairs = []
for annot_path in all_annot_paths:
    basename = os.path.splitext(os.path.basename(annot_path))[0]
    if basename in image_basenames:
        image_path = os.path.join(SOURCE_IMAGES_DIR, basename + '.jpg')
        valid_pairs.append({'xml': annot_path, 'img': image_path})

# Shuffle and split the list of VERIFIED pairs
random.seed(42)
random.shuffle(valid_pairs)
split_idx = int(len(valid_pairs) * 0.8)
train_pairs = valid_pairs[:split_idx]
val_pairs = valid_pairs[split_idx:]

# The Conversion Function and Processing Loop
CLASS_MAPPING = {'MR': 0, 'NC': 1, 'WF': 2}
def convert_and_copy(pairs, img_dest, label_dest):
    for pair in tqdm(pairs, desc=f"Processing {os.path.basename(img_dest)} set"):
        img = Image.open(pair['img'])
        width, height = img.size
        shutil.copy(pair['img'], img_dest)

        tree = ET.parse(pair['xml'])
        root = tree.getroot()
        yolo_lines = []
        for obj in root.findall('object'):
            class_id = CLASS_MAPPING[obj.find('name').text]
            bndbox = obj.find('bndbox')
            xmin, ymin, xmax, ymax = [float(bndbox.find(t).text) for t in ['xmin', 'ymin', 'xmax', 'ymax']]
            x_center, y_center = (xmin + xmax) / 2 / width, (ymin + ymax) / 2 / height
            width_norm, height_norm = (xmax - xmin) / width, (ymax - ymin) / height
            yolo_lines.append(f"{class_id} {x_center:.6f} {y_center:.6f} {width_norm:.6f} {height_norm:.6f}")

        base_name = os.path.splitext(os.path.basename(pair['img']))[0]
        with open(os.path.join(label_dest, base_name + '.txt'), 'w') as f:
            f.write("\n".join(yolo_lines))

convert_and_copy(train_pairs, PATHS['train_images'], PATHS['train_labels'])
convert_and_copy(val_pairs, PATHS['val_images'], PATHS['val_labels'])

# Create the dataset.yaml file
dataset_yaml_content = {
    'path': YOLO_DATASET_DIR, 'train': 'images/train', 'val': 'images/val',
    'names': { 0: 'MR', 1: 'NC', 2: 'WF' }
}
YAML_PATH = '/content/dataset.yaml'
with open(YAML_PATH, 'w') as f:
    yaml.dump(dataset_yaml_content, f, default_flow_style=False)
print("Dataset preparation is complete")

# Model Training

print("\nStarting YOLOv8 Model Training")

# Load a pretrained YOLOv8 model
model = YOLO('yolov8l.pt')

# Train the model
results = model.train(
    data=YAML_PATH,
    epochs=100,
    patience=15,
    imgsz=800,
    batch=8,
    warmup_epochs=3,
    augment=True,
    cache=True,
    degrees=10,
    scale=0.9,
    shear=2.0,
    perspective=0.0005,
    hsv_h=0.015,
    hsv_s=0.7,
    hsv_v=0.4,
    flipud=0.0,
    fliplr=0.5,
    val=True,
    project='YOLOv8_Insect_Detection',
    name='definitive_working_run',
    exist_ok=True,
    plots=True
)
print("\nTraining complete")

#Standard Metrics, Training Curves & Confusion Matrix

# Load the Best Trained Model
model = YOLO('/content/YOLOv8_Insect_Detection/definitive_working_run/weights/best.pt')


# Run Validation to Calculate Metrics
metrics = model.val(split='val') # use the validation set

print("\nPer-Class Metrics Table")

# Get the class names from the model's configuration
class_names = model.names

# Print the header for the table
print(f"{'Class':<15}{'Precision':>12}{'Recall':>12}{'mAP@.50':>12}{'mAP@.50-.95':>15}")
print("-" * 70)

# Get the lists of metrics directly from the metrics.box object
p_per_class = metrics.box.p      # Precision for each class
r_per_class = metrics.box.r      # Recall for each class
map50_per_class = metrics.box.ap50  # mAP@0.50 for each class
map_per_class = metrics.box.ap   # mAP@0.50-0.95 for each class

# Loop through each class and print its specific metrics
for i, name in class_names.items():
    # The index 'i' corresponds to the class ID
    print(f"{name:<15}{p_per_class[i]:>12.3f}{r_per_class[i]:>12.3f}{map50_per_class[i]:>12.3f}{map_per_class[i]:>15.3f}")

# Also print the overall 'all class' summary metrics, which are calculated by the .mean_results() method
mp, mr, map50, map95 = metrics.box.mean_results()
print("-" * 70)
print(f"{'all':<15}{mp:>12.3f}{mr:>12.3f}{map50:>12.3f}{map95:>15.3f}")


# Display Auto-Generated Plots
RESULTS_DIR = '/content/YOLOv8_Insect_Detection/definitive_working_run/'
print("\n\n--- Training & Validation Curves ---")
display(IPImage(filename=f'{RESULTS_DIR}/results.png', width=900))
print("\n--- Confusion Matrix ---")
display(IPImage(filename=f'{RESULTS_DIR}/confusion_matrix.png', width=600))

# Advanced Visual Analysis (True Positives & False Positives)

# This helper function calculates the Intersection over Union (IoU) between two boxes
# IoU is the core metric for deciding if a detection is correct
def calculate_iou(box1, box2):
    x1, y1, x2, y2 = box1
    x1_gt, y1_gt, x2_gt, y2_gt = box2
    xi1, yi1 = max(x1, x1_gt), max(y1, y1_gt)
    xi2, yi2 = min(x2, x2_gt), min(y2, y2_gt)

    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
    box1_area = (x2 - x1) * (y2 - y1)
    box2_area = (x2_gt - x1_gt) * (y2 - y1_gt)
    union_area = box1_area + box2_area - inter_area

    return inter_area / union_area if union_area != 0 else 0

# Select a random test image and run the analysis
# A new 'test' directory for this purpose to use unseen data
TEST_DIR_IMG = os.path.join(YOLO_DATASET_DIR, 'images/test')
TEST_DIR_LBL = os.path.join(YOLO_DATASET_DIR, 'labels/test')

if not os.path.exists(TEST_DIR_IMG):
    # If the test set wasn't created in the YOLO structure, it must be created
    os.makedirs(TEST_DIR_IMG)
    os.makedirs(TEST_DIR_LBL)
    test_pairs = val_pairs # As a fallback, the validation pairs if test set is empty is used
    convert_and_copy(test_pairs, TEST_DIR_IMG, TEST_DIR_LBL)

random_image_path = random.choice(glob(os.path.join(TEST_DIR_IMG, '*.jpg')))
# Find the original XML path to get GT boxes for comparison
base_name_for_xml = os.path.splitext(os.path.basename(random_image_path))[0]
xml_path = os.path.join('/content/yellow-sticky-traps-dataset/annotations', base_name_for_xml + '.xml')

print(f"Running Advanced Visual Analysis on: {os.path.basename(random_image_path)}")

# Load image with OpenCV for drawing
image = cv2.imread(random_image_path)

# Get YOLOv8 predictions
results = model(random_image_path, verbose=False)
preds = results[0].boxes.data.cpu().numpy()

# Get ground truth boxes from the original XML file
tree = ET.parse(xml_path)
gt_boxes = []
class_names_map = {v: k for k, v in CLASS_MAPPING.items()}
for obj in tree.getroot().findall("object"):
    class_id = CLASS_MAPPING[obj.find("name").text]
    bndbox = obj.find("bndbox")
    gt_boxes.append([float(bndbox.find(t).text) for t in ['xmin', 'ymin', 'xmax', 'ymax']] + [class_id])

# Logic to determine TP, FP, FN
IOU_THRESHOLD = 0.5
gt_boxes_matched = [False] * len(gt_boxes)

# Check each prediction against all ground truth boxes
for pred in preds:
    x1, y1, x2, y2, conf, class_id = pred
    best_iou = 0
    match_idx = -1
    for i, gt_box in enumerate(gt_boxes):
        iou = calculate_iou(pred[:4], gt_box[:4])
        # A match requires high IoU and the same class ID
        if iou > best_iou and class_id == gt_box[4]:
            best_iou = iou
            match_idx = i

    # If a match is found, it's a True Positive (Green)
    if best_iou > IOU_THRESHOLD and not gt_boxes_matched[match_idx]:
        color = (0, 255, 0) # Green for TP
        label = f"TP: {class_names_map[int(class_id)]} {conf:.2f}"
        gt_boxes_matched[match_idx] = True
    # Otherwise, it's a False Positive (Red)
    else:
        color = (255, 0, 0) # Red for FP
        label = f"FP: {class_names_map[int(class_id)]} {conf:.2f}"
    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
    cv2.putText(image, label, (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

# Check for any ground truth boxes that were not matched (False Negatives - Blue)
for i, gt_box in enumerate(gt_boxes):
    if not gt_boxes_matched[i]:
        x1, y1, x2, y2, class_id = gt_box
        color = (0, 0, 255) # Blue for FN
        label = f"FN: {class_names_map[int(class_id)]}"
        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)

# Display the analyzed image
plt.figure(figsize=(15, 15))
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.title("Visual Analysis (Green: True Positive, Red: False Positive, Blue: False Negative)")
plt.show()

# Histogram of Counting Errors

# Using validation set for this analysis
image_files = glob(os.path.join(PATHS['val_images'], '*.jpg'))
counting_errors = {name: [] for name in CLASS_MAPPING.keys()}

# Loop through all images in the validation directory
for image_file in tqdm(image_files, desc="Processing validation images"):
    xml_file = os.path.join(SOURCE_ANNOTATIONS_DIR, os.path.basename(image_file).replace('.jpg', '.xml'))

    # Get Ground Truth Counts from the original XML
    gt_counts = {name: 0 for name in CLASS_MAPPING.keys()}
    if os.path.exists(xml_file):
        tree = ET.parse(xml_file)
        for obj in tree.getroot().findall("object"):
            class_name = obj.find("name").text
            if class_name in gt_counts:
                gt_counts[class_name] += 1

    # Get Predicted Counts from YOLOv8 model
    pred_counts = {name: 0 for name in CLASS_MAPPING.keys()}
    results = model(image_file, conf=0.4, verbose=False) # Confidence threshold
    for box in results[0].boxes:
        class_id = int(box.cls)
        class_name = class_names_map[class_id]
        if class_name in pred_counts:
            pred_counts[class_name] += 1

    # Calculate and store the error (Predicted - Actual) for each class
    for class_name in gt_counts:
        error = pred_counts[class_name] - gt_counts[class_name]
        counting_errors[class_name].append(error)

# Plotting the Histograms
print("\nPlotting Histograms of Counting Errors")
fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)
fig.suptitle('Distribution of Counting Errors per Class (Predicted - Actual)', fontsize=16)

for i, (class_name, errors) in enumerate(counting_errors.items()):
    ax = axes[i]
    sns.histplot(errors, ax=ax, bins=max(10, len(set(errors))), kde=True)
    ax.set_title(f'Class: {class_name}')
    ax.set_xlabel('Counting Error (0 = Perfect Count)')
    ax.axvline(0, color='r', linestyle='--') # Add a line at zero error for reference

axes[0].set_ylabel('Frequency')
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Inference Speed Comparison (CPU vs. GPU)

# 1. Define the timing function
def time_inference(model, image, num_inferences=50):
    # Warm-up run (first inference is always slower)
    _ = model(image, verbose=False)

    # Measurement loop
    start_time = time.time()
    for _ in range(num_inferences):
        _ = model(image, verbose=False)
    end_time = time.time()

    # Calculate Frames Per Second (FPS)
    return num_inferences / (end_time - start_time)

# 2. Run tests on both devices
print("Running Inference Speed Comparison")
# Use a fixed test image for consistent results
test_image_path = random.choice(glob(os.path.join(PATHS['val_images'], '*.jpg')))
test_image = Image.open(test_image_path)
fps_results = {}

# GPU Test (if a GPU is available in the Colab runtime)
if torch.cuda.is_available():
    model_gpu = YOLO('/content/YOLOv8_Insect_Detection/definitive_working_run/weights/best.pt')
    print("Testing on GPU...")
    fps_gpu = time_inference(model_gpu, test_image)
    fps_results['GPU'] = fps_gpu
    print(f"GPU FPS: {fps_gpu:.2f}")
else:
    print("GPU not available")
    fps_results['GPU'] = 0

# CPU Test
model_cpu = YOLO('/content/YOLOv8_Insect_Detection/definitive_working_run/weights/best.pt')
model_cpu.to('cpu')
print("\nTesting on CPU...")
fps_cpu = time_inference(model_cpu, test_image)
fps_results['CPU'] = fps_cpu
print(f"CPU FPS: {fps_cpu:.2f}")

# 3. Plot Comparison Chart
plt.figure(figsize=(8, 6))
bars = plt.bar(fps_results.keys(), fps_results.values(), color=['#4CAF50', '#F44336'])
plt.title('Inference Speed Comparison: CPU vs. GPU')
plt.ylabel('Frames Per Second (FPS)')
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}', va='bottom', ha='center') # add text on top
plt.show()